{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c99fd13",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Assignment 2: Classification and locally weighted regression\n",
    "</center>\n",
    "</h1>\n",
    "<center>\n",
    "CS 4262/5262 - Foundations of Machine Learning<br>\n",
    "Vanderbilt University, Spring 2023<br>\n",
    "Due: Check Brightspace\n",
    "</center>\n",
    "<hr>\n",
    "<br>This assignment will focus on logistic regression (for binary classification) and locally weighted linear regression. For each algorithm, we have provided a class framework as a suggestion, but you are not required to use those in your implementation. Please use good programming practices - include informative comments and vectorize operations whenever possible. In addition to programming tasks, there are short-answer questions throughout the notebook. \n",
    "\n",
    "Contact: Quan Liu quan.liu@vanderbilt.edu for any clarifying questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885f7ec",
   "metadata": {},
   "source": [
    "### Please enter your name:  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a28ea1",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 0: Data\n",
    "\n",
    "\n",
    "You will be applying binary classification to two different datasets: the [Iris](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) dataset and the wine quality dataset (Data Source :https://archive.ics.uci.edu/ml/datasets/Wine+Quality). The Iris dataset is smaller and simpler, and therefore may be useful for debugging. This dataset consists of measurements (septal and petal length and width) of 50 samples from each of 3 species of Iris flower. The wine quality dataset is more complex, and the classification task is to predict whether a sample should be red wine or white wine given the feature.\n",
    "\n",
    "**Task 1**\n",
    "- Load the Iris dataset from scikit-learn. (refer to [link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html))\n",
    "- Here, we will represent each sample by 2 of the 4 available features: petal width and petal length. \n",
    "- Display a scatterplot of the data, such that: \n",
    "    * the x- and y- axes correspond to the two features (petal width, petal length)\n",
    "    * the axes are labelled \n",
    "    * points are colored according to class membership\n",
    "    * the legend describes which iris type (class) is represented by each color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd411f3",
   "metadata": {},
   "source": [
    "**Question 1:  Which classes appear to be linearly separable in this feature space?**\n",
    "\n",
    "Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204819d",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "- Load the wine dataset given in the brightspace.\n",
    "    * we have 1600 lines of white wine data and 1599 lines of red wine data\n",
    "    * white/red wine is labeled as 0/1\n",
    "    * each sample has 11 dimensions of features with the same order as [fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol] and one dimension of label\n",
    "- Here, we will represent each sample by 3 features (using mpl_toolkits.mplot3d.Axes3D): \n",
    "- Similar to the Iris dataset, display a scatterplot of the data such that: \"volatile acidity\", \"fixed acidity\", and \"residual sugar\".\n",
    "    * the x-, y-, and z- axes correspond to the features\n",
    "    * the axes are labelled \n",
    "    * the sample point is colored based on the class\n",
    "    * the legend specifies the label associated with each color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - wine quality dataset\n",
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56df175",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "There are many dimensions of the features, use `sns.PairGrid()` to plot out the pairwise feature relationship on both iris and wine dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91422e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pairwise plot on 2 dataset\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ba204",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "\n",
    " 2.1 Comment on the plots from the wine dataset, compared to the kinds of plots you saw in the Iris dataset. What similarities or differences do you see? What does the PairGrid visualization help to do?\n",
    "\n",
    "Response: \n",
    "\n",
    " 2.2 Discuss separability of the wine dataset, based on what you have seen so far. Is it separable in two features? Do you think multiple features would change this outcome?\n",
    "\n",
    "Response: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7251",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Logistic Regression\n",
    "\n",
    "The first classification algorithm you will implement is Logistic Regression (for binary classification). You do not have to use the class framework provided below, but please make sure to organize and comment your code clearly. \n",
    "\n",
    "**Task 4**\n",
    "Write a LogisticRegression class such that:\n",
    " - parameters ($\\theta$) are optimized using gradient descent \n",
    " - there is an `evaluate` method that returns the model's accuracy on a given set of data\n",
    " - there is a `learning curve` method that plots the cost function against the number of iterations\n",
    " - there is a `decision boundary` method that renders a plot of the training data with the decision boundary overlayed (note: this code is provided for you below - make sure you understand how it works) \n",
    " - please vectorize operations as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - implement LogisticRegression class\n",
    "\n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, X, y, theta, alpha):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.theta = theta \n",
    "        self.alpha = alpha\n",
    "    \n",
    "    #  h (hypothesis): returns p(y=1|x) on inputs contained in the design matrix X\n",
    "    def sigmoid(self, X): \n",
    "        return \n",
    "    \n",
    "    # return predictions of class membership (0,1) of the datapoints in an input matrix X\n",
    "    def predict(self, X):\n",
    "        return\n",
    "    \n",
    "    # cost function J()\n",
    "    def cost(self):\n",
    "        return \n",
    "    \n",
    "    # update theta \n",
    "    def gradient_descent_step(self):\n",
    "        return\n",
    "    \n",
    "    # define a convergence criterion \n",
    "    # run gradient descent until convergence \n",
    "    def run_gradient_descent(self):\n",
    "        return\n",
    "    \n",
    "    # return the model's accuracy on an input (X,y) dataset \n",
    "    def evaluate(self, X, y):\n",
    "        return\n",
    "    \n",
    "    # plot cost function over num gradient descent steps\n",
    "    def learning_curve(self):\n",
    "        return\n",
    "    \n",
    "    # plot decision boundary, based on current model parameters\n",
    "    # you may edit or add cases to this, to accommodate plotting the Iris data too\n",
    "    def decision_boundary(self, dset):\n",
    "        X = self.X[:,1:]\n",
    "        theta = [t[0] for t in self.theta]\n",
    "        y = np.reshape(self.y, (-1))\n",
    "        xax = [np.min(X[:, 0]), np.max(X[:, 0])]\n",
    "        yax = -1.0*(theta[0] + np.dot(theta[1], xax)) / theta[2]\n",
    "        plt.scatter(x=X[y==0,0],y=X[y==0,1],c='red',edgecolor='black')\n",
    "        plt.scatter(x=X[y==1,0],y=X[y==1,1],c='blue',edgecolor='black')\n",
    "        plt.plot(xax, yax)\n",
    "        if dset=='wine':\n",
    "            plt.legend(['red','white', 'decision boundary'])\n",
    "            plt.xlabel('')  # name it as the your input x- and y-\n",
    "            plt.ylabel('')\n",
    "            plt.title('Wine')\n",
    "        elif dset=='iris':\n",
    "            plt.legend(['decision boundary','setosa','versicolor'])\n",
    "            plt.xlabel('petal width')\n",
    "            plt.ylabel('petal length')\n",
    "            plt.title(\"Iris Dataset\")            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea698e",
   "metadata": {},
   "source": [
    "**Task 5**\n",
    "Verify that your method works on the Iris dataset. The Iris dataset is originally a 3-class dataset, but for this purpose, please select two of the 3 classes on which to perform binary classification (and again, use the 2 features \"petal length\" and \"petal width\"). You do not have to split this dataset further into training and testing sets.\n",
    " - Display the decision boundary, superimposed on the scatterplot of the data\n",
    " - Add/modify the `decision_boundary` function if needed to accommodate changes in plotting for the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d3d68",
   "metadata": {},
   "source": [
    "**Task 6**\n",
    "Explore your method on the wine dataset, expanding from 2 dimensions into multiple dimensions.\n",
    " - Split the wine dataset into a training set and a test set (80/20 split). We recommend shuffling the data first.\n",
    " - Then, perform feature scaling (standardizing to mean = 0 and variance = 1) on both the training and test sets. Please write your own function to perform this standardization, rather than using a module from scikit-learn. Note that it is recommended to calculate the scaling parameters (mean and variance) from the training set, and then apply those same paramters to scale the test set, so that the test set does not influence the training in any way. \n",
    " - **we are not expecting to get 100% accuracy on any of the feature combinations**, but an empirical lower bound for the accuracy is given. That is to say, your approach is probably right, as long as your performance on the test set is higher than the number.\n",
    " - Train your model on the wine training data with the following 4 [feature combinations] : percentage to beat during test\n",
    "   * [fixed acidity, volatile acidity, residual sugar] : 85%\n",
    "   * [density, pH, alcohol] : 75%\n",
    "   * [fixed acidity, volatile acidity, chlorides] : 85%\n",
    "   * [all 11 features]: 95%\n",
    "   * note that the features list is: fixed acidity/volatile acidity/citric acid/residual sugar /chlorides/free sulfur dioxide/total sulfur dioxide/density/pH/sulphates/alcohol\n",
    " - Display the decision boundary plots (plot in 2d, so please just choose any 2 of your features as x- and y-). \n",
    " - Display plots of the learning curve \n",
    " - Report the model's final accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Task 6, apply your method to the wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792bce9",
   "metadata": {},
   "source": [
    "**Question 3:**\n",
    "\n",
    " 3.1. Describe the convergence condition you selected.\n",
    "\n",
    "Response:\n",
    "\n",
    " 3.2. What was the model's training accuracy on the Iris dataset (for the two classes you selected)?\n",
    "\n",
    "Response:\n",
    "\n",
    " 3.3. What was the model's training and test accuracy on the wine quality dataset? Which one gives the best performance? Does that live up to your expectation and why?\n",
    "\n",
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18365f33",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Locally Weighted Linear Regression \n",
    "\n",
    "In this second part, you will write a locally weighted linear regression class, and apply it to a synthetic dataset. This dataset is included as a text file on Brightspace, and is called 'LWR_samples.npy'. Each line of the text file represents one training example in the format $x^{(i)},y^{(i)}$ (i.e. the delimiter is a comma). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31972b",
   "metadata": {},
   "source": [
    "#### **Task 7**\n",
    "- Load the synthetic data, from the file `assignment2_LWR_samples.npy`\n",
    "- Interpret the $(x^{(i)},y^{(i)})$ pairs, and plot them with a scatter plot.\n",
    "- Implement a LocallyWeightedLR class (example framework below). To make a prediction at input $x$, weight each training example according to the function we discussed in lecture: \n",
    "$$ w^{(i)} = \\exp\\big(-\\frac{(x^{(i)} - x)^2}{2\\tau^2} \\big), $$\n",
    "where $\\tau$ is a bandwidth parameter that you will experiment with.\n",
    "- To compute the local linear regression parameters ($\\theta$) at each query point, use the closed-form solution. The formula is:\n",
    "$$ \\theta = (X^TWX)^{-1} X^TWy, $$\n",
    "where $X$ is the design matrix formed by your training inputs (make sure to include the intercept term), $W$ is a diagonal matrix whose $i^{th}$ diagonal entry corresponds to the weight of the $i^{th}$ training example (which depend on the point at which you are making a prediction), and $y$ is a column vector containing the target labels of the training examples.\n",
    "\n",
    "- Run this regression model to make predictions at the specific input points x = 4, x = 0.5, and x = -3. Use $\\tau$ = 0.5. Report the values of the local regression parameters $\\theta$ obtained for each of these 3 points.\n",
    "- Now, generate an array of predictions corresponding to equally spaced input points (in the range of [-4.5, 4.5] in steps of 0.05), again using $\\tau$ = 0.5. Generate a plot showing the predictions from Locally Weighted Linear Regression on each of these input points, superimposed on (and colored differently from) the training data.\n",
    "- Repeat the previous step, now using bandwidth parameters $\\tau = 0.1$ and $\\tau = 1.5$. Plot the results, again superimposed on the training data (and in a different color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Implement Locally-Weighted Linear Regression class\n",
    "\n",
    "class LocallyWeightedLR():\n",
    "    \n",
    "    def __init__(self, X, y, tau):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tau = tau \n",
    "        \n",
    "    # use bandwidth variable tau to compute weights for each training point.  \n",
    "    # return a diagonal matrix with w_i on the diagonal (for vectorization)\n",
    "    # note that the values of w_i depend upon the value of the input query point x.\n",
    "    def compute_weights(self, x):\n",
    "        return \n",
    "    \n",
    "    # analytical solution for the local linear regression parameters at the input query point x.\n",
    "    # this should involve calling the above method compute_weights.\n",
    "    def compute_theta(self):\n",
    "        return \n",
    "    \n",
    "    # prediction for an input x\n",
    "    # also return the local linear regression parameters (theta) for this x.\n",
    "    def predict(self, x):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Read in the artificial dataset, plot it, and run the code according to the above instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af99ad",
   "metadata": {},
   "source": [
    "**Question 4**: \n",
    " - Do the local linear regression parameters $\\theta$ returned for the 3 input points (4, 0.5, -3) agree with what you expect, based on the training data in the neighborhood of those points? Why or why not?\n",
    " \n",
    "Response: \n",
    "\n",
    "\n",
    "**Question 5:**  \n",
    " - Based on your observations, describe the effect of increasing and decreasing $\\tau$, in the context of over/underfitting.\n",
    " \n",
    "Response: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c9f77",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Submission \n",
    "\n",
    "Please upload a clean version of your work to Brightspace by the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114cdf2",
   "metadata": {},
   "source": [
    "Below, please acknowledge your collaborators as well as any resources/references (beyond guides to Python syntax) that you have used in this assignment:"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
